{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Ingest ESPN Athletes (Bronze)\n",
        "\n",
        "- Collect athlete IDs from bronze fights\n",
        "- Fetch Core v2 athlete profiles (expand + deref)\n",
        "- Write bronze table `espn_athletes`\n",
        "- Show sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "import json, time\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "try:\n",
        "    dbutils.widgets.text(\"storage_account\", \"storagetmufc\")\n",
        "    dbutils.widgets.text(\"secret_scope\", \"kv-scope\")\n",
        "    dbutils.widgets.text(\"key_name\", \"adls-account-key\")\n",
        "    dbutils.widgets.text(\"db_name\", \"ufc_bronze\")\n",
        "    dbutils.widgets.text(\"max_concurrency\", \"5\")\n",
        "    dbutils.widgets.text(\"http_timeout_sec\", \"30\")\n",
        "    dbutils.widgets.text(\"http_retries\", \"3\")\n",
        "    dbutils.widgets.text(\"user_agent\", \"ufc-pipeline/1.0 (+databricks)\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "storage_account  = dbutils.widgets.get(\"storage_account\")\n",
        "secret_scope     = dbutils.widgets.get(\"secret_scope\")\n",
        "key_name         = dbutils.widgets.get(\"key_name\")\n",
        "DB_NAME          = dbutils.widgets.get(\"db_name\")\n",
        "MAX_CONC         = int(dbutils.widgets.get(\"max_concurrency\"))\n",
        "HTTP_TIMEOUT     = int(dbutils.widgets.get(\"http_timeout_sec\"))\n",
        "HTTP_RETRIES     = int(dbutils.widgets.get(\"http_retries\"))\n",
        "USER_AGENT       = dbutils.widgets.get(\"user_agent\")\n",
        "\n",
        "account_key = dbutils.secrets.get(secret_scope, key_name)\n",
        "spark.conf.set(f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\", account_key)\n",
        "\n",
        "def abfss(container, path=\"\"):\n",
        "    base = f\"abfss://{container}@{storage_account}.dfs.core.windows.net\"\n",
        "    return f\"{base}/{path}\".rstrip(\"/\")\n",
        "\n",
        "PATH_ATHLETES_DELTA = abfss(\"bronze\", f\"{DB_NAME}/espn_athletes\")\n",
        "PATH_LANDING_ROOT   = abfss(\"landing\",\"espn/ufc/athletes\")\n",
        "PATH_RAW_ROOT       = abfss(\"raw\",    \"espn/ufc/athletes\")\n",
        "\n",
        "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "INGESTION_DATE = datetime.now(timezone.utc).date().isoformat()\n",
        "\n",
        "for p in [f\"{PATH_LANDING_ROOT}/run_id={RUN_ID}\", f\"{PATH_RAW_ROOT}/ingestion_date={INGESTION_DATE}\"]:\n",
        "    dbutils.fs.mkdirs(p)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "PATH_EVENTS_RAW = abfss(\"raw\", \"espn/ufc/events\")\n",
        "\n",
        "df_events = spark.read.json(f\"{PATH_EVENTS_RAW}/ingestion_date=*/event_*.json\")\n",
        "\n",
        "refs_df = (df_events\n",
        "    .select(F.explode(F.col(\"competitions\")).alias(\"comp\"))\n",
        "    .select(F.explode(F.col(\"comp.competitors\")).alias(\"cmp\"))\n",
        "    .select(F.col(\"cmp.athlete.$ref\").alias(\"ref\"))\n",
        "    .where(F.col(\"ref\").isNotNull())\n",
        ")\n",
        "athlete_ids_df = refs_df.select(F.regexp_extract(F.col(\"ref\"), r\"/athletes/(\\d+)\", 1).alias(\"athlete_id\")).where(F.col(\"athlete_id\") != \"\").distinct()\n",
        "athlete_ids = [r[\"athlete_id\"] for r in athlete_ids_df.collect()]\n",
        "print(f\"Found unique athlete IDs: {len(athlete_ids)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read athlete IDs from upstream task (fallback to RAW scan)\n",
        "try:\n",
        "    ids_json = dbutils.jobs.taskValues.get(taskKey=\"ingest_fights\", key=\"athlete_ids_json\", debugValue=\"[]\")\n",
        "    upstream_ids = json.loads(ids_json) if ids_json else []\n",
        "except Exception:\n",
        "    upstream_ids = []\n",
        "\n",
        "if upstream_ids:\n",
        "    athlete_ids = upstream_ids\n",
        "    print(f\"Using upstream athlete IDs: {len(athlete_ids)}\")\n",
        "else:\n",
        "    print(\"No upstream IDs found. Falling back to RAW scan (distinct).\")\n",
        "    PATH_EVENTS_RAW = abfss(\"raw\", \"espn/ufc/events\")\n",
        "    df_events = spark.read.json(f\"{PATH_EVENTS_RAW}/ingestion_date=*/event_*.json\")\n",
        "    refs_df = (df_events\n",
        "        .select(F.explode(F.col(\"competitions\")).alias(\"comp\"))\n",
        "        .select(F.explode(F.col(\"comp.competitors\")).alias(\"cmp\"))\n",
        "        .select(F.col(\"cmp.athlete.$ref\").alias(\"ref\"))\n",
        "        .where(F.col(\"ref\").isNotNull())\n",
        "    )\n",
        "    athlete_ids_df = refs_df.select(F.regexp_extract(F.col(\"ref\"), r\"/athletes/(\\d+)\", 1).alias(\"athlete_id\")).where(F.col(\"athlete_id\") != \"\").distinct()\n",
        "    athlete_ids = [r[\"athlete_id\"] for r in athlete_ids_df.collect()]\n",
        "    print(f\"Found unique athlete IDs via RAW: {len(athlete_ids)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch athlete profiles (Core v2)\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "HEADERS = {\"User-Agent\": USER_AGENT, \"Accept\": \"application/json\"}\n",
        "BASE_ATHLETE = \"https://sports.core.api.espn.com/v2/sports/mma/athletes/{athleteId}\"\n",
        "EXPAND = {\n",
        "    \"lang\": \"en\", \"region\": \"us\", \"contentorigin\": \"espn\",\n",
        "    \"expand\": \"records,teams,athlete,statistics\"\n",
        "}\n",
        "\n",
        "def http_get(url, params=None, timeout=HTTP_TIMEOUT, retries=HTTP_RETRIES, backoff=1.5):\n",
        "    last_exc=None\n",
        "    for i in range(retries+1):\n",
        "        try:\n",
        "            r = requests.get(url, params=params, headers=HEADERS, timeout=timeout)\n",
        "            if r.status_code == 200:\n",
        "                return r.json()\n",
        "            if r.status_code in (429,500,502,503,504):\n",
        "                raise requests.HTTPError(str(r.status_code))\n",
        "            r.raise_for_status()\n",
        "        except Exception as e:\n",
        "            last_exc=e\n",
        "            if i<retries:\n",
        "                time.sleep(backoff**i)\n",
        "            else:\n",
        "                raise last_exc\n",
        "\n",
        "profiles = {}\n",
        "if athlete_ids:\n",
        "    with ThreadPoolExecutor(max_workers=min(MAX_CONC, len(athlete_ids))) as ex:\n",
        "        futs = {ex.submit(http_get, BASE_ATHLETE.format(athleteId=aid), EXPAND): aid for aid in athlete_ids}\n",
        "        for f in as_completed(futs):\n",
        "            aid = futs[f]\n",
        "            try:\n",
        "                profiles[aid] = f.result()\n",
        "            except Exception as e:\n",
        "                profiles[aid] = {\"error\": str(e)}\n",
        "\n",
        "print(f\"Fetched profiles: {len(profiles)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Landing and raw mirror\n",
        "base = f\"{PATH_LANDING_ROOT}/run_id={RUN_ID}\"\n",
        "for aid, payload in profiles.items():\n",
        "    dbutils.fs.put(f\"{base}/athlete_{aid}.json\", json.dumps(payload, ensure_ascii=False), True)\n",
        "print(f\"landing saved: {len(profiles)} files\")\n",
        "\n",
        "src = f\"{PATH_LANDING_ROOT}/run_id={RUN_ID}\"\n",
        "dst = f\"{PATH_RAW_ROOT}/ingestion_date={INGESTION_DATE}\"\n",
        "try:\n",
        "    dbutils.fs.cp(src, dst, recurse=True)\n",
        "except Exception:\n",
        "    for f in dbutils.fs.ls(src):\n",
        "        dbutils.fs.cp(f.path, f\"{dst}/{f.name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bronze table and upsert\n",
        "spark.sql(f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS hive_metastore.{DB_NAME}.espn_athletes (\n",
        "  athlete_id STRING,\n",
        "  full_name STRING,\n",
        "  display_name STRING,\n",
        "  birth_date DATE,\n",
        "  country STRING,\n",
        "  height STRING,\n",
        "  height_cm INT,\n",
        "  reach_cm INT,\n",
        "  stance STRING,\n",
        "  weight_class STRING,\n",
        "  team STRING,\n",
        "  combat_style STRING,\n",
        "  image_url STRING,\n",
        "  ingestion_date DATE,\n",
        "  run_id STRING,\n",
        "  raw_payload STRING\n",
        ") USING DELTA LOCATION '{PATH_ATHLETES_DELTA}'\n",
        "\"\"\")\n",
        "\n",
        "# Helpers reusing HTTP for optional $ref deref\n",
        "ref_cache = {}\n",
        "\n",
        "def ref_get(url: str):\n",
        "    if not isinstance(url, str):\n",
        "        return {}\n",
        "    if url in ref_cache:\n",
        "        return ref_cache[url]\n",
        "    try:\n",
        "        data = http_get(url)\n",
        "        ref_cache[url] = data\n",
        "        return data\n",
        "    except Exception:\n",
        "        ref_cache[url] = {}\n",
        "        return {}\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_country(p: dict) -> str | None:\n",
        "    if isinstance(p.get(\"citizenship\"), str) and p.get(\"citizenship\"):\n",
        "        return p.get(\"citizenship\")\n",
        "    assoc = p.get(\"association\")\n",
        "    if isinstance(assoc, dict):\n",
        "        loc = assoc.get(\"location\")\n",
        "        if isinstance(loc, dict) and isinstance(loc.get(\"country\"), str):\n",
        "            return loc.get(\"country\")\n",
        "    bp = p.get(\"birthPlace\")\n",
        "    if isinstance(bp, dict):\n",
        "        if \"$ref\" in bp:\n",
        "            d = ref_get(bp[\"$ref\"]) or {}\n",
        "            return d.get(\"country\") or d.get(\"name\")\n",
        "        return bp.get(\"country\") or bp.get(\"name\")\n",
        "    cc = p.get(\"citizenshipCountry\")\n",
        "    if isinstance(cc, dict):\n",
        "        return cc.get(\"alt\") or cc.get(\"name\")\n",
        "    return None\n",
        "\n",
        "def _parse_display_height_inches(txt: str) -> float | None:\n",
        "    try:\n",
        "        m = re.match(r\"^(\\d+)\\s*'\\s*(\\d+)?\\s*\\\"?\", txt.strip())\n",
        "        if not m:\n",
        "            return None\n",
        "        feet = int(m.group(1)); inches = int(m.group(2) or 0)\n",
        "        return feet * 12 + inches\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def extract_display_height_str(p: dict) -> str | None:\n",
        "    dh = p.get(\"displayHeight\")\n",
        "    if isinstance(dh, str) and dh.strip():\n",
        "        return dh.strip()\n",
        "    h = p.get(\"height\")\n",
        "    try:\n",
        "        inches = float(h) if isinstance(h, (int, float)) else None\n",
        "        if inches is not None:\n",
        "            feet = int(inches // 12); rest = int(round(inches - feet * 12))\n",
        "            return f\"{feet}' {rest}\\\"\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def extract_height_cm(p: dict) -> int | None:\n",
        "    v = p.get(\"heightCentimeters\")\n",
        "    if isinstance(v, (int, float)): return int(v)\n",
        "    h = p.get(\"height\")\n",
        "    if isinstance(h, dict):\n",
        "        v = h.get(\"centimeters\") or h.get(\"cm\")\n",
        "        if isinstance(v, (int, float)): return int(v)\n",
        "    if isinstance(h, (int, float)):\n",
        "        return int(round(float(h) * 2.54))\n",
        "    dh = p.get(\"displayHeight\")\n",
        "    if isinstance(dh, str):\n",
        "        inches = _parse_display_height_inches(dh)\n",
        "        if isinstance(inches, (int, float)):\n",
        "            return int(round(float(inches) * 2.54))\n",
        "    meas = p.get(\"measurements\")\n",
        "    if isinstance(meas, list):\n",
        "        for m in meas:\n",
        "            if isinstance(m, dict) and str(m.get(\"type\")).lower() == \"height\":\n",
        "                v = m.get(\"value\") or m.get(\"centimeters\")\n",
        "                if isinstance(v, (int, float)):\n",
        "                    return int(v)\n",
        "    return None\n",
        "\n",
        "def extract_reach_cm(p: dict) -> int | None:\n",
        "    v = p.get(\"reachCentimeters\")\n",
        "    if isinstance(v, (int, float)): return int(v)\n",
        "    r = p.get(\"reach\")\n",
        "    if isinstance(r, dict):\n",
        "        v = r.get(\"centimeters\") or r.get(\"cm\")\n",
        "        if isinstance(v, (int, float)): return int(v)\n",
        "    if isinstance(r, (int, float)):\n",
        "        return int(round(float(r) * 2.54))\n",
        "    dr = p.get(\"displayReach\")\n",
        "    if isinstance(dr, str):\n",
        "        try:\n",
        "            v = float(dr.replace('\"','').strip())\n",
        "            return int(round(v * 2.54))\n",
        "        except Exception:\n",
        "            pass\n",
        "    meas = p.get(\"measurements\")\n",
        "    if isinstance(meas, list):\n",
        "        for m in meas:\n",
        "            if isinstance(m, dict) and str(m.get(\"type\")).lower() == \"reach\":\n",
        "                v = m.get(\"value\") or m.get(\"centimeters\")\n",
        "                if isinstance(v, (int, float)):\n",
        "                    return int(v)\n",
        "    return None\n",
        "\n",
        "def extract_stance(p: dict) -> str | None:\n",
        "    st = p.get(\"stance\")\n",
        "    if isinstance(st, dict):\n",
        "        return st.get(\"text\") or st.get(\"name\")\n",
        "    if isinstance(st, str):\n",
        "        return st\n",
        "    return p.get(\"fightingStyle\") or None\n",
        "\n",
        "def extract_team(p: dict) -> str | None:\n",
        "    assoc = p.get(\"association\")\n",
        "    if isinstance(assoc, dict):\n",
        "        return assoc.get(\"name\")\n",
        "    return None\n",
        "\n",
        "def extract_combat_style(p: dict) -> str | None:\n",
        "    styles = p.get(\"styles\")\n",
        "    if isinstance(styles, list):\n",
        "        texts = []\n",
        "        for s in styles:\n",
        "            if isinstance(s, dict):\n",
        "                t = s.get(\"text\") or s.get(\"name\")\n",
        "                if t: texts.append(str(t))\n",
        "        if texts:\n",
        "            return \", \".join(sorted(set(texts)))\n",
        "    return None\n",
        "\n",
        "# Parse profiles into rows\n",
        "rows = []\n",
        "for aid, p in profiles.items():\n",
        "    if not isinstance(p, dict):\n",
        "        continue\n",
        "    full_name = p.get(\"fullName\") or p.get(\"name\")\n",
        "    display_name = p.get(\"displayName\")\n",
        "    birth_date = p.get(\"birthDate\") or p.get(\"dateOfBirth\")\n",
        "    country = extract_country(p)\n",
        "    height_str = extract_display_height_str(p)\n",
        "    height_cm = extract_height_cm(p)\n",
        "    reach_cm = extract_reach_cm(p)\n",
        "    stance = extract_stance(p)\n",
        "    wc = p.get(\"weightClass\")\n",
        "    weight_class = wc.get(\"text\") or wc.get(\"shortName\") if isinstance(wc, dict) else None\n",
        "    team = extract_team(p)\n",
        "    combat_style = extract_combat_style(p)\n",
        "    image_url = (p.get(\"headshot\") or {}).get(\"href\") if isinstance(p.get(\"headshot\"), dict) else None\n",
        "    rows.append((aid, full_name, display_name, birth_date, country, height_str, height_cm, reach_cm, stance, weight_class, team, combat_style, image_url, INGESTION_DATE, RUN_ID, json.dumps(p, ensure_ascii=False)))\n",
        "\n",
        "schema = \"athlete_id STRING, full_name STRING, display_name STRING, birth_date STRING, country STRING, height STRING, height_cm INT, reach_cm INT, stance STRING, weight_class STRING, team STRING, combat_style STRING, image_url STRING, ingestion_date STRING, run_id STRING, raw_payload STRING\"\n",
        "df = spark.createDataFrame(rows, schema)\n",
        "df = (df\n",
        "    .withColumn(\"birth_date\", F.to_date(\"birth_date\"))\n",
        "    .withColumn(\"ingestion_date\", F.to_date(\"ingestion_date\"))\n",
        ")\n",
        "\n",
        "# Upsert (avoid disk staging; use temp view)\n",
        "view_name = f\"_s_athletes_{RUN_ID}\"\n",
        "df.createOrReplaceTempView(view_name)\n",
        "spark.sql(f\"\"\"\n",
        "MERGE INTO hive_metastore.{DB_NAME}.espn_athletes t\n",
        "USING (SELECT * FROM {view_name}) s\n",
        "ON t.athlete_id = s.athlete_id\n",
        "WHEN MATCHED THEN UPDATE SET *\n",
        "WHEN NOT MATCHED THEN INSERT *\n",
        "\"\"\")\n",
        "spark.catalog.dropTempView(view_name)\n",
        "\n",
        "print(f\"Upserted athletes: {df.count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "# Sample athletes (Python to avoid ${DB_NAME} SQL interpolation issues)\n",
        "display(spark.table(f\"hive_metastore.{DB_NAME}.espn_athletes\").orderBy(F.col(\"full_name\")).limit(20))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
