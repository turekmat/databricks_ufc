{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ingest ESPN UFC: Events & Fights (Bronze)\n",
        "\n",
        "Incremental/idempotent ingestion from ESPN Core/Scoreboard APIs into bronze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# ===== WIDGETS =====\n",
        "dbutils.widgets.text(\"storage_account\", \"storagetmufc\")\n",
        "dbutils.widgets.text(\"secret_scope\", \"kv-scope\")\n",
        "dbutils.widgets.text(\"key_name\", \"adls-account-key\")\n",
        "dbutils.widgets.dropdown(\"run_mode\", \"backfill\", [\"backfill\",\"incremental\"])\n",
        "dbutils.widgets.text(\"years_back\", \"7\")\n",
        "dbutils.widgets.text(\"overlap_days\", \"1\")\n",
        "dbutils.widgets.text(\"max_concurrency\", \"5\")\n",
        "dbutils.widgets.text(\"http_timeout_sec\", \"30\")\n",
        "dbutils.widgets.text(\"http_retries\", \"3\")\n",
        "dbutils.widgets.text(\"db_name\", \"ufc_bronze\")\n",
        "dbutils.widgets.text(\"user_agent\", \"ufc-pipeline/1.0 (+databricks)\")\n",
        "dbutils.widgets.text(\"estimate_sample_days\", \"12\")\n",
        "dbutils.widgets.text(\"estimate_sample_core_events\", \"20\")\n",
        "\n",
        "storage_account  = dbutils.widgets.get(\"storage_account\")\n",
        "secret_scope     = dbutils.widgets.get(\"secret_scope\")\n",
        "key_name         = dbutils.widgets.get(\"key_name\")\n",
        "run_mode         = dbutils.widgets.get(\"run_mode\")\n",
        "years_back       = int(dbutils.widgets.get(\"years_back\"))\n",
        "overlap_days     = int(dbutils.widgets.get(\"overlap_days\"))\n",
        "MAX_CONC         = int(dbutils.widgets.get(\"max_concurrency\"))\n",
        "HTTP_TIMEOUT     = int(dbutils.widgets.get(\"http_timeout_sec\"))\n",
        "HTTP_RETRIES     = int(dbutils.widgets.get(\"http_retries\"))\n",
        "DB_NAME          = dbutils.widgets.get(\"db_name\")\n",
        "USER_AGENT       = dbutils.widgets.get(\"user_agent\")\n",
        "EST_SAMPLE_DAYS  = int(dbutils.widgets.get(\"estimate_sample_days\"))\n",
        "EST_SAMPLE_CORE  = int(dbutils.widgets.get(\"estimate_sample_core_events\"))\n",
        "\n",
        "# ===== STORAGE CONFIG (Access Key) =====\n",
        "account_key = dbutils.secrets.get(secret_scope, key_name)\n",
        "spark.conf.set(f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\", account_key)\n",
        "\n",
        "def abfss(container, path=\"\"):\n",
        "    base = f\"abfss://{container}@{storage_account}.dfs.core.windows.net\"\n",
        "    return f\"{base}/{path}\".rstrip(\"/\")\n",
        "\n",
        "# ===== PATHS =====\n",
        "PATH_EVENTS_DELTA = abfss(\"bronze\", \"ufc_bronze/espn_events\")\n",
        "PATH_FIGHTS_DELTA = abfss(\"bronze\", \"ufc_bronze/espn_fights\")\n",
        "PATH_META_WM      = abfss(\"meta\",   \"ufc/pipeline/watermarks/last_event_date\")\n",
        "PATH_META_RUNS    = abfss(\"meta\",   \"ufc/pipeline/runs\")\n",
        "PATH_LOGS_RUN     = abfss(\"logs\",   \"ufc/pipeline\")\n",
        "PATH_LANDING_ROOT = abfss(\"landing\",\"espn/ufc/events\")\n",
        "PATH_RAW_ROOT     = abfss(\"raw\",    \"espn/ufc/events\")\n",
        "\n",
        "# ===== BACKFILL CLEANUP (nuke-and-pave for conflicting data) =====\n",
        "if run_mode == \"backfill\":\n",
        "    print(\"Backfill mode detected → wiping bronze events/fights and watermark to start fresh\")\n",
        "    # Ensure database exists, then drop tables if present\n",
        "    try:\n",
        "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS hive_metastore.{DB_NAME}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    for tbl in [\"espn_events\", \"espn_fights\", \"ufc_last_event_date\"]:\n",
        "        try:\n",
        "            spark.sql(f\"DROP TABLE IF EXISTS hive_metastore.{DB_NAME}.{tbl}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    # Remove underlying locations (Delta paths and watermark path)\n",
        "    for p in [PATH_EVENTS_DELTA, PATH_FIGHTS_DELTA, PATH_META_WM]:\n",
        "        try:\n",
        "            dbutils.fs.rm(p, recurse=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# ===== RUNTIME & BOOTSTRAP (bez vytváření Delta LOCATION složek) =====\n",
        "from datetime import datetime, timezone, date, timedelta\n",
        "import json, time, math\n",
        "from statistics import mean\n",
        "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "INGESTION_DATE = date.today().isoformat()\n",
        "\n",
        "def ensure_dir(path):\n",
        "    dbutils.fs.mkdirs(path)\n",
        "    try: dbutils.fs.put(f\"{path}/.keep\",\"\", overwrite=False)\n",
        "    except: pass\n",
        "\n",
        "for p in [\n",
        "    f\"{PATH_LANDING_ROOT}/run_id={RUN_ID}\",\n",
        "    f\"{PATH_RAW_ROOT}/ingestion_date={INGESTION_DATE}\",\n",
        "    f\"{PATH_LOGS_RUN}/run_id={RUN_ID}\",\n",
        "]: ensure_dir(p)\n",
        "\n",
        "# ===== CLEANUP non-Delta složek pro LOCATION =====\n",
        "targets = [PATH_META_WM, PATH_META_RUNS, PATH_EVENTS_DELTA, PATH_FIGHTS_DELTA]\n",
        "def is_delta(path):\n",
        "    try: return any(f.name.rstrip('/') == \"_delta_log\" for f in dbutils.fs.ls(path))\n",
        "    except: return False\n",
        "for p in targets:\n",
        "    try:\n",
        "        if not is_delta(p) and len(dbutils.fs.ls(p))>0:\n",
        "            dbutils.fs.rm(p, recurse=True)\n",
        "    except: pass\n",
        "\n",
        "# ===== ESPN HELPERS =====\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "BASE_SCOREBOARD = \"https://site.api.espn.com/apis/site/v2/sports/mma/ufc/scoreboard\"\n",
        "CORE_EVENT_TPL  = \"https://sports.core.api.espn.com/v2/sports/mma/leagues/ufc/events/{eventId}\"\n",
        "HEADERS = {\"User-Agent\": USER_AGENT, \"Accept\": \"application/json\"}\n",
        "CORE_PARAMS = {\n",
        "    \"lang\": \"en\",\n",
        "    \"region\": \"us\",\n",
        "    \"contentorigin\": \"espn\",\n",
        "    \"enable\": \"competitions,competitors,venues,notes,rankings,statistics\",\n",
        "    \"expand\": \"competitions,competitions.competitors,competitions.competitors.athlete,competitions.competitors.statistics,competitions.venue,competitions.status\",\n",
        "}\n",
        "\n",
        "def http_get(url, params=None, timeout=HTTP_TIMEOUT, retries=HTTP_RETRIES, backoff=1.5):\n",
        "    last_exc=None\n",
        "    for i in range(retries+1):\n",
        "        try:\n",
        "            r = requests.get(url, params=params, headers=HEADERS, timeout=timeout)\n",
        "            if r.status_code == 200: return r.json()\n",
        "            if r.status_code in (429,500,502,503,504): raise requests.HTTPError(f\"{r.status_code}\")\n",
        "            r.raise_for_status()\n",
        "        except Exception as e:\n",
        "            last_exc=e\n",
        "            if i<retries: time.sleep(backoff**i)\n",
        "            else: raise last_exc\n",
        "\n",
        "def date_range_backfill(years):\n",
        "    end = date.today(); start = end - timedelta(days=int(365.25*years))\n",
        "    d=start\n",
        "    while d<=end: \n",
        "        yield d; d += timedelta(days=1)\n",
        "\n",
        "def date_range_incremental(from_date, overlap):\n",
        "    start = from_date - timedelta(days=overlap)\n",
        "    end = date.today() + timedelta(days=7)\n",
        "    d=start\n",
        "    while d<=end:\n",
        "        yield d; d += timedelta(days=1)\n",
        "\n",
        "# ===== WATERMARK (Delta, external LOCATION) =====\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS hive_metastore.{DB_NAME}\")\n",
        "spark.sql(f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS hive_metastore.{DB_NAME}.ufc_last_event_date (\n",
        "  last_event_date DATE, updated_at TIMESTAMP\n",
        ") USING DELTA LOCATION '{PATH_META_WM}'\n",
        "\"\"\")\n",
        "\n",
        "def load_watermark():\n",
        "    df = spark.table(f\"hive_metastore.{DB_NAME}.ufc_last_event_date\")\n",
        "    return None if df.count()==0 else df.select(\"last_event_date\").first()[0]\n",
        "\n",
        "def save_watermark(dt):\n",
        "    df = spark.createDataFrame([(dt, datetime.now(timezone.utc))], \"last_event_date DATE, updated_at TIMESTAMP\")\n",
        "    df.coalesce(1).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(PATH_META_WM)\n",
        "\n",
        "# ===== IO HELPERS =====\n",
        "def put_json(path, obj): dbutils.fs.put(path, json.dumps(obj, ensure_ascii=False), overwrite=True)\n",
        "\n",
        "def write_landing(day, scoreboard_json, event_payloads):\n",
        "    base = f\"{PATH_LANDING_ROOT}/run_id={RUN_ID}\"\n",
        "    dstr = day.strftime('%Y%m%d')\n",
        "    put_json(f\"{base}/scoreboard_{dstr}.json\", scoreboard_json)\n",
        "    for eid, ev in event_payloads.items():\n",
        "        put_json(f\"{base}/event_{eid}.json\", ev)\n",
        "        # print only event date DONE\n",
        "        try:\n",
        "            node = ev.get(\"header\") if isinstance(ev, dict) and isinstance(ev.get(\"header\"), dict) else ev\n",
        "            ev_dt = (node.get(\"date\") if isinstance(node, dict) else None) or (\n",
        "                node.get(\"competitions\", [{}])[0].get(\"date\") if isinstance(node, dict) else None\n",
        "            )\n",
        "            if isinstance(ev_dt, str) and len(ev_dt) >= 10:\n",
        "                ev_str = ev_dt[:10]\n",
        "            else:\n",
        "                ev_str = dstr\n",
        "        except Exception:\n",
        "            ev_str = dstr\n",
        "        print(f\"event {ev_str} done\")\n",
        "\n",
        "def mirror_to_raw():\n",
        "    src = f\"{PATH_LANDING_ROOT}/run_id={RUN_ID}\"\n",
        "    dst = f\"{PATH_RAW_ROOT}/ingestion_date={INGESTION_DATE}\"\n",
        "    try:\n",
        "        dbutils.fs.cp(src, dst, recurse=True)\n",
        "    except Exception:\n",
        "        for f in dbutils.fs.ls(src):\n",
        "            dbutils.fs.cp(f.path, f\"{dst}/{f.name}\", recurse=True)\n",
        "\n",
        "# ===== PARSING =====\n",
        "def dig(d, path, default=None):\n",
        "    cur=d\n",
        "    try:\n",
        "        for p in path:\n",
        "            cur = cur[p] if isinstance(p,int) else cur.get(p)\n",
        "            if cur is None: return default\n",
        "        return cur\n",
        "    except: return default\n",
        "\n",
        "def extract_event_row(ev, ref_get=None):\n",
        "    node = ev.get(\"header\") if isinstance(ev, dict) and isinstance(ev.get(\"header\"), dict) else ev\n",
        "    return {\n",
        "        \"event_id\": str(dig(node,[\"id\"],\"\")),\n",
        "        \"event_date\": dig(node,[\"date\"],None) or dig(node,[\"competitions\",0,\"date\"],None),\n",
        "        \"event_name\": dig(node,[\"name\"],None) or dig(ev,[\"header\",\"shortName\"],None) or dig(node,[\"competitions\",0,\"name\"],None) or dig(node,[\"competitions\",0,\"notes\",0,\"headline\"],None),\n",
        "        \"venue\": dig(node,[\"venue\",\"fullName\"],None) or dig(node,[\"competitions\",0,\"venue\",\"fullName\"],None),\n",
        "        \"country\": dig(node,[\"venue\",\"address\",\"country\"],None) or dig(node,[\"competitions\",0,\"venue\",\"address\",\"country\"],None),\n",
        "        \"status\": dig(node,[\"status\",\"type\",\"name\"],None) or dig(node,[\"status\",\"name\"],None) or dig(node,[\"competitions\",0,\"status\",\"type\",\"name\"],None) or dig(node,[\"competitions\",0,\"status\",\"name\"],None),\n",
        "        \"num_fights\": len(dig(node,[\"competitions\"],[]) or []),\n",
        "        \"raw_payload\": json.dumps(ev, ensure_ascii=False)\n",
        "    }\n",
        "\n",
        "def extract_fight_rows(ev, ref_get=None):\n",
        "    rows=[]\n",
        "    node = ev.get(\"header\") if isinstance(ev, dict) and isinstance(ev.get(\"header\"), dict) else ev\n",
        "    event_id=str(dig(node,[\"id\"],\"\")); event_date=dig(node,[\"date\"],None) or dig(node,[\"competitions\",0,\"date\"],None)\n",
        "    comps_raw = dig(node,[\"competitions\"],[]) or []\n",
        "\n",
        "    def maybe_deref(obj):\n",
        "        if ref_get and isinstance(obj,dict) and obj.get(\"$ref\") and isinstance(obj.get(\"$ref\"),str):\n",
        "            try: return ref_get(obj[\"$ref\"]) or {}\n",
        "            except: return {}\n",
        "        return obj\n",
        "\n",
        "    for comp0 in comps_raw:\n",
        "        comp = maybe_deref(comp0)\n",
        "        comp_id = str(dig(comp,[\"id\"],\"\"))\n",
        "        bout    = dig(comp,[\"name\"],None) or dig(comp,[\"notes\",0,\"headline\"],None)\n",
        "        wclass  = dig(comp,[\"type\",\"text\"],None)\n",
        "        if not wclass:\n",
        "            wnode = maybe_deref(comp.get(\"weightClass\") or comp.get(\"weight\") or {})\n",
        "            wclass = dig(wnode,[\"name\"],None)\n",
        "        order   = dig(comp,[\"matchNumber\"],None)\n",
        "        if order is None: order = dig(comp,[\"order\"],None)\n",
        "        s_node  = maybe_deref(comp.get(\"status\") or {})\n",
        "        status  = dig(s_node,[\"type\",\"name\"],None) or dig(s_node,[\"name\"],None)\n",
        "\n",
        "        comp_competitors = comp.get(\"competitors\")\n",
        "        if isinstance(comp_competitors, dict) and comp_competitors.get(\"$ref\"):\n",
        "            cc = maybe_deref(comp_competitors)\n",
        "            cs = cc.get(\"items\") or cc or []\n",
        "        else:\n",
        "            cs = comp_competitors or []\n",
        "\n",
        "        fa, fb = {}, {}\n",
        "        if len(cs)>=1:\n",
        "            c = maybe_deref(cs[0])\n",
        "            anode = maybe_deref(c.get(\"athlete\") or {})\n",
        "            stats_node = c.get(\"statistics\")\n",
        "            if isinstance(stats_node, dict) and stats_node.get(\"$ref\"):\n",
        "                sn = maybe_deref(stats_node)\n",
        "                stats_list = sn.get(\"stats\") or sn.get(\"items\") or []\n",
        "            else:\n",
        "                stats_list = stats_node or []\n",
        "            fa={\"name\": dig(anode,[\"displayName\"],None) or dig(anode,[\"fullName\"],None),\n",
        "                \"win\": dig(c,[\"winner\"],None),\n",
        "                \"stats\": {str(dig(s,[\"name\"],\"\") or dig(s,[\"shortDisplayName\"],\"\")).strip():\n",
        "                           str(dig(s,[\"displayValue\"],\"\") or dig(s,[\"value\"],\"\")).strip()\n",
        "                           for s in (stats_list or []) if (dig(s,[\"name\"],\"\") or dig(s,[\"shortDisplayName\"],\"\"))}}\n",
        "        if len(cs)>=2:\n",
        "            c = maybe_deref(cs[1])\n",
        "            anode = maybe_deref(c.get(\"athlete\") or {})\n",
        "            stats_node = c.get(\"statistics\")\n",
        "            if isinstance(stats_node, dict) and stats_node.get(\"$ref\"):\n",
        "                sn = maybe_deref(stats_node)\n",
        "                stats_list = sn.get(\"stats\") or sn.get(\"items\") or []\n",
        "            else:\n",
        "                stats_list = stats_node or []\n",
        "            fb={\"name\": dig(anode,[\"displayName\"],None) or dig(anode,[\"fullName\"],None),\n",
        "                \"win\": dig(c,[\"winner\"],None),\n",
        "                \"stats\": {str(dig(s,[\"name\"],\"\") or dig(s,[\"shortDisplayName\"],\"\")).strip():\n",
        "                           str(dig(s,[\"displayValue\"],\"\") or dig(s,[\"value\"],\"\")).strip()\n",
        "                           for s in (stats_list or []) if (dig(s,[\"name\"],\"\") or dig(s,[\"shortDisplayName\"],\"\"))}}\n",
        "\n",
        "        rows.append({\n",
        "            \"competition_id\": comp_id, \"event_id\": event_id, \"event_date\": event_date,\n",
        "            \"bout_name\": bout, \"weight_class\": wclass,\n",
        "            \"card_order\": int(order) if str(order).isdigit() else None, \"status\": status,\n",
        "            \"fighter_a_name\": fa.get(\"name\"), \"fighter_a_winner\": (bool(fa.get(\"win\")) if fa.get(\"win\") is not None else None),\n",
        "            \"fighter_a_stats\": json.dumps(fa.get(\"stats\",{}), ensure_ascii=False),\n",
        "            \"fighter_b_name\": fb.get(\"name\"), \"fighter_b_winner\": (bool(fb.get(\"win\")) if fb.get(\"win\") is not None else None),\n",
        "            \"fighter_b_stats\": json.dumps(fb.get(\"stats\",{}), ensure_ascii=False),\n",
        "            \"raw_payload\": json.dumps(comp, ensure_ascii=False)\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "# ===== FETCH & PARSE PER DAY (šetří paměť) =====\n",
        "def fetch_day(day):\n",
        "    sb = http_get(BASE_SCOREBOARD, params={\"dates\": day.strftime(\"%Y%m%d\")})\n",
        "    events = sb.get(\"events\",[]) or []\n",
        "    ids = [str(e.get(\"id\")) for e in events if e.get(\"id\") is not None]\n",
        "    payloads={}\n",
        "    if MAX_CONC<=1 or len(ids)<=1:\n",
        "        for eid in ids: payloads[eid]=http_get(CORE_EVENT_TPL.format(eventId=eid), params=CORE_PARAMS)\n",
        "    else:\n",
        "        with ThreadPoolExecutor(max_workers=min(MAX_CONC,len(ids))) as ex:\n",
        "            futs={ex.submit(http_get, CORE_EVENT_TPL.format(eventId=eid), CORE_PARAMS): eid for eid in ids}\n",
        "            for f in as_completed(futs):\n",
        "                eid=futs[f]\n",
        "                try: payloads[eid]=f.result()\n",
        "                except Exception as e: put_json(f\"{PATH_LOGS_RUN}/run_id={RUN_ID}/error_event_{eid}.json\", {\"error\":str(e)})\n",
        "    write_landing(day, sb, payloads)\n",
        "    erows, frows = [], []\n",
        "    # simple per-day ref cache to minimize repeated HTTP calls\n",
        "    ref_cache={}\n",
        "    def ref_get(url):\n",
        "        if not isinstance(url,str): return {}\n",
        "        if url in ref_cache: return ref_cache[url]\n",
        "        try:\n",
        "            data = http_get(url)\n",
        "            ref_cache[url]=data; return data\n",
        "        except Exception:\n",
        "            ref_cache[url]={}; return {}\n",
        "    for ev in payloads.values():\n",
        "        erows.append(extract_event_row(ev, ref_get))\n",
        "        frows.extend(extract_fight_rows(ev, ref_get))\n",
        "    return erows, frows\n",
        "\n",
        "# ===== ESTIMATION =====\n",
        "def _percentile(values, p):\n",
        "    vs = sorted(values)\n",
        "    if not vs: return None\n",
        "    k = (len(vs)-1) * p\n",
        "    f = math.floor(k); c = math.ceil(k)\n",
        "    if f == c: return vs[int(k)]\n",
        "    return vs[f] + (vs[c]-vs[f]) * (k - f)\n",
        "\n",
        "def _pick_sample_days(days_list, k):\n",
        "    n = len(days_list)\n",
        "    if n <= k: return list(days_list)\n",
        "    if k <= 1: return [days_list[0]]\n",
        "    idxs = {round(i * (n-1) / (k-1)) for i in range(k)}\n",
        "    return [days_list[i] for i in sorted(idxs)]\n",
        "\n",
        "def estimate_workload(days_list, sample_days=12, sample_core=20):\n",
        "    sample_days = max(1, min(sample_days, len(days_list)))\n",
        "    sampled_days = _pick_sample_days(days_list, sample_days)\n",
        "\n",
        "    sb_times, sb_sizes, events_per_day = [], [], []\n",
        "    sampled_event_ids = []\n",
        "\n",
        "    for d in sampled_days:\n",
        "        t0 = time.perf_counter()\n",
        "        sb = http_get(BASE_SCOREBOARD, params={\"dates\": d.strftime(\"%Y%m%d\")})\n",
        "        sb_times.append(max(0.0, time.perf_counter() - t0))\n",
        "        try:\n",
        "            sb_sizes.append(len(json.dumps(sb, ensure_ascii=False)))\n",
        "        except Exception:\n",
        "            sb_sizes.append(0)\n",
        "        evs = sb.get(\"events\", []) or []\n",
        "        events_per_day.append(len(evs))\n",
        "        for e in evs:\n",
        "            if len(sampled_event_ids) < sample_core and e.get(\"id\") is not None:\n",
        "                sampled_event_ids.append(str(e.get(\"id\")))\n",
        "        if len(sampled_event_ids) >= sample_core:\n",
        "            break\n",
        "\n",
        "    core_times, core_sizes, fights_per_event = [], [], []\n",
        "    for eid in sampled_event_ids:\n",
        "        t0 = time.perf_counter()\n",
        "        ev = http_get(CORE_EVENT_TPL.format(eventId=eid))\n",
        "        core_times.append(max(0.0, time.perf_counter() - t0))\n",
        "        try:\n",
        "            core_sizes.append(len(json.dumps(ev, ensure_ascii=False)))\n",
        "        except Exception:\n",
        "            core_sizes.append(0)\n",
        "        comps = (ev.get(\"competitions\") or []) if isinstance(ev, dict) else []\n",
        "        fights_per_event.append(len(comps))\n",
        "\n",
        "    days_total = len(days_list)\n",
        "    events_per_day_avg = float(mean(events_per_day)) if events_per_day else 0.0\n",
        "    fights_per_event_avg = float(mean(fights_per_event)) if fights_per_event else 0.0\n",
        "    est_events_total = int(round(events_per_day_avg * days_total))\n",
        "    est_requests_total = int(days_total + est_events_total)\n",
        "\n",
        "    sb_avg_s = float(mean(sb_times)) if sb_times else 0.3\n",
        "    core_avg_s = float(mean(core_times)) if core_times else 0.4\n",
        "    sb_p90_s = float(_percentile(sb_times, 0.9) or sb_avg_s)\n",
        "    core_p90_s = float(_percentile(core_times, 0.9) or core_avg_s)\n",
        "\n",
        "    conc = max(1, int(MAX_CONC))\n",
        "    batches_per_day_avg = math.ceil((events_per_day_avg or 0.0) / conc) if events_per_day_avg else 0\n",
        "\n",
        "    eta_s_avg = days_total * (sb_avg_s + batches_per_day_avg * core_avg_s)\n",
        "    eta_s_p90 = days_total * (sb_p90_s + batches_per_day_avg * core_p90_s)\n",
        "\n",
        "    sb_avg_bytes = int(mean(sb_sizes)) if sb_sizes else 0\n",
        "    core_avg_bytes = int(mean(core_sizes)) if core_sizes else 0\n",
        "    est_total_bytes = days_total * sb_avg_bytes + est_events_total * core_avg_bytes\n",
        "\n",
        "    return {\n",
        "        \"days_total\": days_total,\n",
        "        \"sample_days\": len(sampled_days),\n",
        "        \"sampled_core_events\": len(sampled_event_ids),\n",
        "        \"avg_events_per_day\": round(events_per_day_avg, 2),\n",
        "        \"avg_fights_per_event\": round(fights_per_event_avg, 2) if fights_per_event else None,\n",
        "        \"estimated_events_total\": est_events_total,\n",
        "        \"estimated_requests_total\": est_requests_total,\n",
        "        \"max_concurrency\": conc,\n",
        "        \"avg_latency_s\": {\"scoreboard\": round(sb_avg_s, 3), \"core_event\": round(core_avg_s, 3)},\n",
        "        \"p90_latency_s\": {\"scoreboard\": round(sb_p90_s, 3), \"core_event\": round(core_p90_s, 3)},\n",
        "        \"eta_seconds_avg\": int(eta_s_avg),\n",
        "        \"eta_minutes_avg\": round(eta_s_avg/60.0, 1),\n",
        "        \"eta_minutes_p90\": round(eta_s_p90/60.0, 1),\n",
        "        \"avg_payload_bytes\": {\"scoreboard\": sb_avg_bytes, \"core_event\": core_avg_bytes},\n",
        "        \"estimated_total_bytes\": est_total_bytes,\n",
        "        \"estimated_total_megabytes\": round(est_total_bytes/1_000_000.0, 2)\n",
        "    }\n",
        "\n",
        "# ===== BRONZE TABLES (Delta, external LOCATION) =====\n",
        "spark.sql(f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS hive_metastore.{DB_NAME}.espn_events (\n",
        "  event_id STRING, event_date TIMESTAMP, event_year INT, event_name STRING,\n",
        "  venue STRING, country STRING, status STRING, num_fights INT,\n",
        "  ingestion_date DATE, run_id STRING, raw_payload STRING\n",
        ") USING DELTA PARTITIONED BY (event_year) LOCATION '{PATH_EVENTS_DELTA}'\n",
        "\"\"\")\n",
        "spark.sql(f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS hive_metastore.{DB_NAME}.espn_fights (\n",
        "  competition_id STRING, event_id STRING, event_date TIMESTAMP, event_year INT,\n",
        "  bout_name STRING, weight_class STRING, card_order INT, status STRING,\n",
        "  fighter_a_name STRING, fighter_a_winner BOOLEAN, fighter_a_stats STRING,\n",
        "  fighter_b_name STRING, fighter_b_winner BOOLEAN, fighter_b_stats STRING,\n",
        "  ingestion_date DATE, run_id STRING, raw_payload STRING\n",
        ") USING DELTA PARTITIONED BY (event_year) LOCATION '{PATH_FIGHTS_DELTA}'\n",
        "\"\"\")\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def rows_to_df(event_rows, fight_rows):\n",
        "    event_schema_str = \"event_id STRING, event_date STRING, event_name STRING, venue STRING, country STRING, status STRING, num_fights INT, raw_payload STRING\"\n",
        "    fight_schema_str = \"competition_id STRING, event_id STRING, event_date STRING, bout_name STRING, weight_class STRING, card_order INT, status STRING, fighter_a_name STRING, fighter_a_winner BOOLEAN, fighter_a_stats STRING, fighter_b_name STRING, fighter_b_winner BOOLEAN, fighter_b_stats STRING, raw_payload STRING\"\n",
        "    df_e = spark.createDataFrame(event_rows, schema=event_schema_str) if event_rows else spark.createDataFrame([], event_schema_str)\n",
        "    df_f = spark.createDataFrame(fight_rows,  schema=fight_schema_str)  if fight_rows  else spark.createDataFrame([], fight_schema_str)\n",
        "    # Robust ISO8601 parsing for timestamps like 2025-02-15T21:00Z or with seconds\n",
        "    def parse_iso_ts(col):\n",
        "        return F.coalesce(\n",
        "            F.to_timestamp(F.col(col), \"yyyy-MM-dd'T'HH:mm:ssX\"),\n",
        "            F.to_timestamp(F.col(col), \"yyyy-MM-dd'T'HH:mmX\"),\n",
        "            F.to_timestamp(F.regexp_replace(F.col(col), \"Z$\", \"+00:00\"), \"yyyy-MM-dd'T'HH:mm:ssXXX\"),\n",
        "            F.to_timestamp(F.regexp_replace(F.col(col), \"Z$\", \"+00:00\"), \"yyyy-MM-dd'T'HH:mmXXX\"),\n",
        "            F.to_timestamp(F.regexp_replace(F.col(col), \"T\", \" \"), \"yyyy-MM-dd HH:mm:ss\"),\n",
        "            F.to_timestamp(F.regexp_replace(F.col(col), \"T\", \" \"), \"yyyy-MM-dd HH:mm\")\n",
        "        )\n",
        "    df_e = (df_e.withColumn(\"event_ts\", parse_iso_ts(\"event_date\"))\n",
        "                 .withColumn(\"event_year\",F.year(\"event_ts\").cast(\"int\"))\n",
        "                 .withColumn(\"ingestion_date\",F.to_date(F.lit(INGESTION_DATE)))\n",
        "                 .withColumn(\"run_id\",F.lit(RUN_ID))\n",
        "                 .drop(\"event_date\").withColumnRenamed(\"event_ts\",\"event_date\"))\n",
        "    df_f = (df_f.withColumn(\"event_ts\", parse_iso_ts(\"event_date\"))\n",
        "                 .withColumn(\"event_year\",F.year(\"event_ts\").cast(\"int\"))\n",
        "                 .withColumn(\"ingestion_date\",F.to_date(F.lit(INGESTION_DATE)))\n",
        "                 .withColumn(\"run_id\",F.lit(RUN_ID))\n",
        "                 .drop(\"event_date\").withColumnRenamed(\"event_ts\",\"event_date\"))\n",
        "    return df_e, df_f\n",
        "\n",
        "def upsert_bronze(df_e, df_f):\n",
        "    view_e = f\"_s_events_{RUN_ID}\"\n",
        "    view_f = f\"_s_fights_{RUN_ID}\"\n",
        "    df_e.createOrReplaceTempView(view_e)\n",
        "    df_f.createOrReplaceTempView(view_f)\n",
        "    spark.sql(f\"\"\"\n",
        "        MERGE INTO hive_metastore.{DB_NAME}.espn_events t\n",
        "        USING (SELECT * FROM {view_e}) s\n",
        "        ON t.event_id = s.event_id\n",
        "        WHEN MATCHED THEN UPDATE SET *\n",
        "        WHEN NOT MATCHED THEN INSERT *\n",
        "    \"\"\")\n",
        "    spark.sql(f\"\"\"\n",
        "        MERGE INTO hive_metastore.{DB_NAME}.espn_fights t\n",
        "        USING (SELECT * FROM {view_f}) s\n",
        "        ON t.competition_id = s.competition_id AND t.event_id = s.event_id\n",
        "        WHEN MATCHED THEN UPDATE SET *\n",
        "        WHEN NOT MATCHED THEN INSERT *\n",
        "    \"\"\")\n",
        "    spark.catalog.dropTempView(view_e)\n",
        "    spark.catalog.dropTempView(view_f)\n",
        "\n",
        "# ===== RUN =====\n",
        "run_stats={\"days\":0,\"events\":0,\"fights\":0,\"errors\":0}\n",
        "event_rows_all=[]; fight_rows_all=[]\n",
        "\n",
        "try:\n",
        "    # Build concrete list of days for this run\n",
        "    if run_mode==\"backfill\":\n",
        "        days_list = list(date_range_backfill(years_back))\n",
        "    else:\n",
        "        _wm = load_watermark()\n",
        "        days_list = list(date_range_backfill(years_back)) if _wm is None else list(date_range_incremental(_wm, overlap_days))\n",
        "\n",
        "    # Pre-run estimation (lightweight sampling)\n",
        "    try:\n",
        "        estimation = estimate_workload(days_list, EST_SAMPLE_DAYS, EST_SAMPLE_CORE)\n",
        "        put_json(f\"{PATH_LOGS_RUN}/run_id={RUN_ID}/estimation.json\", estimation)\n",
        "        display({\"run_id\": RUN_ID, \"estimation\": estimation})\n",
        "    except Exception as _est_e:\n",
        "        put_json(f\"{PATH_LOGS_RUN}/run_id={RUN_ID}/estimation_error.json\", {\"error\": str(_est_e)})\n",
        "\n",
        "    printed_sample=False\n",
        "    for day in days_list:\n",
        "        try:\n",
        "            erows, frows = fetch_day(day)\n",
        "            event_rows_all.extend(erows); fight_rows_all.extend(frows)\n",
        "            run_stats[\"days\"] += 1\n",
        "            if (not printed_sample) and erows:\n",
        "                try:\n",
        "                    first_event = erows[0]\n",
        "                    feid = first_event.get(\"event_id\")\n",
        "                    fights_for_event = [r for r in frows if r.get(\"event_id\") == feid]\n",
        "                    sample_out = {\n",
        "                        \"sample_event\": first_event,\n",
        "                        \"sample_fights\": fights_for_event[:5],\n",
        "                        \"sample_fights_total\": len(fights_for_event)\n",
        "                    }\n",
        "                    print(json.dumps(sample_out, ensure_ascii=False, indent=2))\n",
        "                except Exception:\n",
        "                    pass\n",
        "                printed_sample=True\n",
        "        except Exception as e:\n",
        "            run_stats[\"errors\"] += 1\n",
        "            put_json(f\"{PATH_LOGS_RUN}/run_id={RUN_ID}/error_day_{day.isoformat()}.json\", {\"error\":str(e)})\n",
        "\n",
        "    mirror_to_raw()\n",
        "    df_e, df_f = rows_to_df(event_rows_all, fight_rows_all)\n",
        "    # Guard against default partition: drop records with null event_date/event_year\n",
        "    df_e_valid = df_e.filter((F.col(\"event_date\").isNotNull()) & (F.col(\"event_year\").isNotNull()))\n",
        "    df_f_valid = df_f.filter((F.col(\"event_date\").isNotNull()) & (F.col(\"event_year\").isNotNull()))\n",
        "    dropped_e = df_e.count() - df_e_valid.count()\n",
        "    dropped_f = df_f.count() - df_f_valid.count()\n",
        "    if dropped_e or dropped_f:\n",
        "        put_json(f\"{PATH_LOGS_RUN}/run_id={RUN_ID}/dropped_null_partitions.json\", {\n",
        "            \"dropped_events\": int(dropped_e),\n",
        "            \"dropped_fights\": int(dropped_f)\n",
        "        })\n",
        "    run_stats[\"events\"] = df_e_valid.count(); run_stats[\"fights\"] = df_f_valid.count()\n",
        "    upsert_bronze(df_e_valid, df_f_valid)\n",
        "    # Refresh table metadata to avoid stale reads\n",
        "    try:\n",
        "        spark.catalog.refreshTable(f\"hive_metastore.{DB_NAME}.espn_events\")\n",
        "        spark.catalog.refreshTable(f\"hive_metastore.{DB_NAME}.espn_fights\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if run_stats[\"events\"]>0:\n",
        "        max_dt = df_e.agg(F.max(\"event_date\")).first()[0]\n",
        "        if max_dt: save_watermark(max_dt.date())\n",
        "\n",
        "    status=\"SUCCESS\"\n",
        "\n",
        "except Exception as e:\n",
        "    status=\"FAILED\"\n",
        "    put_json(f\"{PATH_LOGS_RUN}/run_id={RUN_ID}/fatal_error.json\", {\"error\":str(e)})\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    spark.sql(f\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS hive_metastore.{DB_NAME}.pipeline_runs (\n",
        "      run_id STRING, started_at TIMESTAMP, finished_at TIMESTAMP, run_mode STRING,\n",
        "      years_back INT, overlap_days INT, events_count BIGINT, fights_count BIGINT, errors BIGINT, status STRING\n",
        "    ) USING DELTA LOCATION '{PATH_META_RUNS}'\n",
        "    \"\"\")\n",
        "    started = datetime.strptime(RUN_ID,\"%Y%m%dT%H%M%SZ\").replace(tzinfo=timezone.utc); finished = datetime.now(timezone.utc)\n",
        "    log_df = spark.createDataFrame([(\n",
        "        RUN_ID, started, finished, run_mode, years_back, overlap_days,\n",
        "        int(run_stats[\"events\"]), int(run_stats[\"fights\"]), int(run_stats[\"errors\"]), status\n",
        "    )], schema=\"run_id STRING, started_at TIMESTAMP, finished_at TIMESTAMP, run_mode STRING, years_back INT, overlap_days INT, events_count BIGINT, fights_count BIGINT, errors BIGINT, status STRING\")\n",
        "    log_df.coalesce(1).write.format(\"delta\").mode(\"append\").save(PATH_META_RUNS)\n",
        "\n",
        "display({\"run_id\": RUN_ID, \"status\": status, **run_stats})\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
