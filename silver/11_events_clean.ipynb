{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UFC Events Transform & Clean (Bronze -> Silver)\n",
        "\n",
        "Cleans and writes `espn_events_silver`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "try:\n",
        "    dbutils.widgets.text(\"storage_account\", \"storagetmufc\")\n",
        "    dbutils.widgets.text(\"secret_scope\", \"kv-scope\")\n",
        "    dbutils.widgets.text(\"key_name\", \"adls-account-key\")\n",
        "    dbutils.widgets.text(\"bronze_db\", \"ufc_bronze\")\n",
        "    dbutils.widgets.text(\"silver_db\", \"ufc_silver\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "storage_account = dbutils.widgets.get(\"storage_account\") if 'dbutils' in globals() else None\n",
        "secret_scope = dbutils.widgets.get(\"secret_scope\") if 'dbutils' in globals() else None\n",
        "key_name = dbutils.widgets.get(\"key_name\") if 'dbutils' in globals() else None\n",
        "bronze_db = dbutils.widgets.get(\"bronze_db\") if 'dbutils' in globals() else \"ufc_bronze\"\n",
        "silver_db = dbutils.widgets.get(\"silver_db\") if 'dbutils' in globals() else \"ufc_silver\"\n",
        "\n",
        "try:\n",
        "    account_key = dbutils.secrets.get(secret_scope, key_name)\n",
        "    spark.conf.set(f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\", account_key)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    spark.sql(\"USE CATALOG hive_metastore\")\n",
        "except Exception:\n",
        "    try:\n",
        "        spark.catalog.setCurrentCatalog(\"hive_metastore\")\n",
        "    except Exception:\n",
        "        pass\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {bronze_db}\")\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
        "try:\n",
        "    spark.catalog.setCurrentDatabase(bronze_db)\n",
        "except Exception:\n",
        "    spark.sql(f\"USE DATABASE {bronze_db}\")\n",
        "print(\"Bronze DB:\", bronze_db, \"| Silver DB:\", silver_db)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_events = spark.table(f\"hive_metastore.{bronze_db}.espn_events\")\n",
        "print(\"Events rows:\", _events.count())\n",
        "display(_events.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in _events.columns]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview BEFORE cleaning (events)\n",
        "display(_events.orderBy(F.desc(\"event_date\")).limit(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SPARSE_THRESHOLD = 0.98\n",
        "MANDATORY_EVENTS = {\"event_id\",\"event_date\",\"event_name\"} \n",
        "\n",
        "w = Window.partitionBy(\"event_id\").orderBy(F.desc(\"ingestion_date\"), F.desc(\"run_id\"))\n",
        "e1 = (_events\n",
        "    .withColumn(\"event_name\", F.trim(\"event_name\"))\n",
        "    .withColumn(\"venue\", F.trim(\"venue\"))\n",
        "    .withColumn(\"country\", F.trim(\"country\"))\n",
        "    .withColumn(\"rn\", F.row_number().over(w)).filter(\"rn=1\").drop(\"rn\"))\n",
        "\n",
        "rows_cnt = e1.count()\n",
        "null_stats = [(c, (e1.filter(F.col(c).isNull()).count() if rows_cnt else 0) / rows_cnt if rows_cnt else 0.0) for c in e1.columns]\n",
        "null_map = {c: r for c, r in null_stats}\n",
        "print(\"Null ratios:\", {k: round(v,3) for k,v in null_map.items()})\n",
        "\n",
        "candidate_cols = [\"venue\",\"country\",\"status\",\"num_fights\"]\n",
        "keep_cols = [c for c in candidate_cols if null_map.get(c, 0.0) < SPARSE_THRESHOLD]\n",
        "\n",
        "select_cols = [\n",
        "    \"event_id\",\n",
        "    \"event_date\",\n",
        "    F.year(\"event_date\").alias(\"event_year\"),\n",
        "    \"event_name\",\n",
        "]\n",
        "for c in keep_cols:\n",
        "    select_cols.append(c)\n",
        "\n",
        "out = e1.select(*select_cols)\n",
        "\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {silver_db}\")\n",
        "out.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").saveAsTable(f\"hive_metastore.{silver_db}.espn_events_silver\")\n",
        "print(\"Events silver written; kept columns:\", out.columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview AFTER cleaning (events)\n",
        "display(spark.table(f\"hive_metastore.{silver_db}.espn_events_silver\").orderBy(F.desc(\"event_date\")).limit(20))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
